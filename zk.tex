\chapter{Доказательство с нулевым разглашением}
\section{Интерактивные протоколы}

В этом разделе предполагается знание основ теории алгоритмов. Раздел имеет ознакомительный характер и рассказывает только лишь идею, лежащую в основе понятия доказательства с нулевым разглашением.

Интерактивной машиной Тьюринга (ИМТ) (интерактивным алгоритмом, программой) называется машина Тьюринга, имеющая 1) рабочую ленту для чтения и записи, 2) одностороннюю входную ленту для чтения, 3) одностороннюю входную ленту для чтения, содержащую бесконечную последовательность случайных битов, 4) одностороннюю коммуникационную ленту только для записи, 5) одностороннюю коммуникационную ленту только для чтения. 

Интерактивным протоколом называется упорядоченная пара интерактивных машин Тьюринга $(A,B)$ такая, что 1) они имеют общую ленту для чтения входных данных, 2) коммуникационная лента для записи одной ИМТ является коммуникационной лентой для чтения другой ИМТ и наоборот. В процессе работы ИМТ находятся поочередно в активном и неактивном режиме. В начальный момент времени активна машина $B$. Сообщением одной ИМТ другой называется слово, которое первая пишет на свою коммуникационную ленту для записи. Эта лента является лентой для чтения второй машины. После того, как активная машина написала сообщение другой, она переходит в неактивный режим, а другая, соответственно, переходит в активный. Вычислительное время работы ИМТ считается время её работы в активном режиме. По определению предполагается, что $B$ обладает полиномиальными вычислительными ресурсами, а $A$ обладает неограниченными вычислительными ресурсами (знания фактов в рассматриваемых ниже задачах можно заменить неограниченными вычислительными ресурсами, с помощью которых эти факты можно установить).
Говорят, что $B$ допускает входное слово $w$ при взаимодействии с $A$ (будем записывать этот факт таким образом: $(A,B)(w)=1$), если $B$ останавливается в допускающем заключительном состоянии. Машина $B$ отвергает входное слово $w$, если она останавливается в заключительном состоянии, не являющемся допускающим (будем записывать это так: $(A,B)(w)=0$). Протокол заканчивает работу, когда останавливается машина $B$.


Говорят, что язык $L$ в алфавите $X$, $L\subseteq X^*$, имеет интерактивную систему доказательства, если существует интерактивный протокол $(P,V)$ такой, что 
\begin{enumerate}
	\item для любого слова $w\in L$ вероятность того, что $(P,V)(w)=1$ равна $1$ (свойство полноты),
	\item для любой ИМТ $P'$ и для любого слова $w\notin L$ вероятность того, что $(P',V)(w)=1$ меньше $1/2$ (свойство корректности).
\end{enumerate}
Здесь под $P$ понимается роль доказывающего (Proover), а под $V$ понимается роль проверяющего (Verifier). Цель доказывающего убедить проверяющего, что $w\in L$. Цель проверяющего согласиться с доказывающим, если действительно $w\in L$, и отвергнуть доказательство, если на самом деле $w\notin L$ и доказывающий мошенничает.
Класс всех языков, для которых существует интерактивная система доказательства, обозначается через $IP$.

Обозначим через $VIEW((P,V)(w))$ последовательность сообщений, посылаемых $P$ и $V$ друг другу. Другими словами, $VIEW((A,B)(w))$~-- то, что можно наблюдать в каналах связи в процессе взаимодействия проверяющего и доказывающего. Поскольку $P$ и $V$ пользуются случайными числами, то $VIEW((P,V)(w))$ является случайной величиной.

Если существует вероятностная полиномиальная машина Тьюринга, которая для каждого слова $w\in L$ порождает случайную величину с точно таким же распределением, что и $VIEW((P,V)(w))$, то говорят, что интерактивный протокол $(P,V)$ представляет собой доказательство с совершенным нулевым разглашением.

Если существует вероятностная полиномиальная машина Тьюринга, которая для каждого слова $w\in L$ порождает случайную величину с распределением, неотличимым от распределения случайной величины $VIEW((P,V)(w))$ средствами статистического анализа, то говорят, что интерактивный протокол $(P,V)$ представляет собой доказательство со статистическим нулевым разглашением.
 
Если существует вероятностная полиномиальная машина Тьюринга, которая для каждого слова $w\in L$ порождает случайную величину с распределением, неотличимым от распределения случайной величины $VIEW((P,V)(w))$ никаким полиномиальным алгоритмом, то говорят, что интерактивный протокол $(P,V)$ представляет собой доказательство с вычислительным нулевым разглашением.


%
%\subsection{Изоморфизм графов}
%
%\subsection{Неизоморфизм графов}
%
%\subsection{Гамильтонов обход}
%
%\subsection{$3$-раскраска графа}

\subsection{Протокол Фейге-Фиата-Шамира}
Протокол Фейге-Фиата-Шамира -- протокол аутентификации с нулевым разглашением, основывается на вычислительной сложности решения квадратных сравнений по модулю составного числа.

В протоколе задействованы две стороны обмена информацией $P$ (доказывающий) и $V$ (проверяющий), а также посредник~-- доверенное лицо. Посредник выбирает два простых числа $p$ и $q$ и публикует число $n=pq$. Доказывающий $P$ выбирает $k$ взаимно простых с $n$ чисел $d_1$, $d_2,\ldots,d_k \in Z_n^*$ и вычисляет $e_i=d_i^2\pmod{n}$, $1\le i\le k$.  Набор чисел $(d_1, d_2,\ldots,d_k)$ является закрытым ключом, секретом доказывающего $P$, а $(e_1, e_2,\dots,e_k,n)$ открытым ключом.

Интерактивное доказательство того, что $P$ знает закрытый ключ $(d_1, d_2,\ldots,d_k)$, состоит из следующих шагов.

\begin{enumerate}
	\item $P$ выбирает случайное число $r$, $1\le r\le n-1$,  случайный бит $b\in\{0,1\}$, вычисляет число $x=(-1)^{b}r^2\pmod{n}$ и отправляет $x$ стороне $V$.
	\item $V$ отправляет $P$ набор из $k$ случайных бит $(v_1,v_2,\ldots,v_k)$, $v_i\in\{0,1\}$.
	\item $P$ вычисляет число $y=rd_1^{v_1}d_2^{v_2}\ldots d_k^{v_k}\pmod{n}$ и отправляет его $V$.
	\item $V$ проверяет равенство $y^2=\pm xe_1^{v_1}e_2^{v_2}\ldots e_k^{v_k}\pmod{n}$.
\end{enumerate}
Доказательство принимается, если последнее равенство верно. Вероятность успешного мошенничества со стороны $P$ за один раунд равна $1/2^k$.
Вероятность успешного мошенничества за $t$ раундов равна $1/2^{kt}$.

Модификация протокола Фейге-Фиата-Шамира позволяет использовать его как цифровую подпись. Модификация заключается в одновременном моделировании $t$ раундов исходного протокола, в котором роль $V$ играет хэш-функция. Обозначим подписываемое сообщение через $m$, а хэш-функцию через $h$. Тогда схема цифровой подписи Фейге-Фиата-Шамира выглядит следующим образом.
 
\begin{enumerate}
	\item $P$ выбирает $t$ случайных чисел $r_i$, $1\le r_i\le n-1$, $1\le i\le t$. 
	\item $P$ вычисляет числа $x_i=r_i^2\pmod{n}$, $1\le i\le t$.
	\item $P$ вычисляет значение хэш-функции $h(m.x_1.x_2.\ldots.x_t)$ от конкатенации чисел $m$, $x_1$, $\ldots$, $x_t$, и берет первые $tk$ бит в качестве значений для $v_{11},v_{12},\ldots,v_{1k},\ldots,v_{t1},\ldots,v_{tk}$.
	\item $P$ вычисляет числа $y_i=r_id_1^{v_{i1}}d_2^{v_{i2}}\ldots d_k^{v_{ik}}\pmod{n}$.
\end{enumerate}
Подписью участника $P$ сообщения $m$ является набор чисел: $x_1,\ldots,x_t$, $v_{11},v_{12},\ldots,v_{1k},\ldots,v_{t1},\ldots,v_{tk}$.

Для проверки подписи достаточно вычислить $x'_{i}=y_i^2e_1^{-v_{i1}}e_2^{-v_{i2}}\ldots e_k^{-v_{ik}}$, $1\le i\le t$, и проверить равны ли первые $kt$ битов значения функции $h(m_1.x'_1.\ldots.x'_t)$ числам $v_{11},v_{12},\ldots,v_{1k},\ldots,v_{t1},\ldots,v_{tk}$. 

\subsection{Схема Шнорра}

Схема Шнора -- криптографический протокол аутентификации. В протоколе задействованы два участника обмена информацией $P$ и $V$. Участник $P$ генерирует два ключа: $e$~-- открытый ключ, $d$~-- закрытый ключ. В процессе обмена сообщениями $P$ должен доказать участнику $V$ с помощью ключа $e$, что он знает ключ $d$, тем самым $P$ идентифицирует себя как $P$ (только он знает $d$ и никто другой).

Ключи формируются следующим образом. Выбираются два большим простых числа $p$ и $q$ так, чтобы $q$ являлся делителем $p-1$. Затем: 1) выбирается число $g\in Z_p^*$, $g\neq 1$, такое, что $g^q=1\pmod{p}$, 2) выбирается случайное число $d<q$, 3) вычисляется число $y=g^{-d}\pmod{p}$. Открытым ключом является набор чисел $e=(p,q,g,y)$, а закрытым~-- число $d$.

Протокол аутентификации:
\begin{enumerate}
	\item (этот шаг можно и нужно делать заранее, так как предварительные вычисления ускоряют процесс вычислений при непосредственной аутентификации) $P$ выбирает случайное $r<q$ и вычисляет $x=g^r\pmod{p}$,
	\item $P$ посылает $V$ число $x$, 
	\item $V$ посылает случайное число $v\in \{0,1,\ldots, 2^t-1\}$,
	\item $P$ вычисляет число $w=(r+d\cdot v)\pmod{q}$ и посылает его $V$,
	\item $V$ проверяет равенство $x=g^wy^v$.
\end{enumerate}
Параметр $t$ в алгоритме берётся произвольно, исходя из того, что сложность вскрытия алгоритма приблизительно равна $2^t$.
 
Модификацию с помощью хэш-функции схемы Шнорра используют как протокол цифровой подписи сообщения $m$ участником $P$. Обозначим хэш-функцию через $h$. 

\begin{enumerate}
	\item $P$ выбирает случайное $r<q$ и вычисляет $x=g^r\pmod{p}$.
	\item $P$ с помощью конкатенации чисел $m$ и  $x$ получает число $m.x$, затем вычисляет $v=h(m.x)$.
	\item $P$ вычисляет число $w=(r+d\cdot v)\pmod{q}$. Подписью является пара чисел $w$ и $v$. 
	\item $V$ вычисляет $x'=g^wy^v$ и сравнивает, равны ли $v$ и $h(m.x')$. Если равны, то подпись верна.
\end{enumerate}
 

\chapter{Информационная энтропия}
%\section{Надежность шифров}
\section{Формальные модели шифров}
Для того чтобы что-либо доказывать, необходимы математические модели исследуемых объектов. Введём алгебраическую модель шифра (шифрсистемы, криптосистемы).

\begin{definition} Шифром назовем совокупность $S=(M,C,K,e,d)$, где конечные множества $M$, $C$ и $K$ являются множествами возможных открытых текстов, шифрованных текстов и ключей соответственно,  $e:M\times K\rightarrow C$~-- правило зашифрования и $d:C\times K\rightarrow M$~-- правило расшифрования, удовлетворяющие следующим условиям:
\begin{enumerate}
\item для любых  $m \in M$ и  $k \in K$ выполняется равенство  $d(e(m,k),k)=m$,
\item  $C=\bigcup_{m \in M, k \in K}{e(m,k)}$.
\end{enumerate}
\end{definition}
Другими словами, шифр~-- это совокупность множества возможных открытых текстов, возможных ключей, возможных шифртекстов (криптограмм), правил зашифровывания и правил расшифровывания. Отметим, что условие 1) отвечает требованию однозначности расшифровывания, а условие 2) означает, что любая криптограмма может быть получена из некоторого открытого текста и некоторого ключа. Данное определение отражает основные свойства реальных шифров и для подавляющего большинства известных шифров можно составить такую модель. Однако с  точки зрения криптоанализа более интересна вероятностная модель шифра, в которой сообщения $M$ и ключи $K$ рассматриваются как случайные величины $\xi_M$ и $\xi_K$, значениями которых являются сообщения и ключи, соответственно.
Пусть $F_M$,  $F_K$~-- априорные функции распределения случайных величин $\xi_M$ и $\xi_K$. Это значит, что для любых  $m \in M$ и  $k\in K$ определены вероятности  $F_M(m)$ и  $F_K(k)$ того, что перехваченная криптоаналитиком криптограмма получена из открытого сообщения $m$ с помощью ключа $k$. Тогда  $S=(M,C,K,e,d,F_M,F_K)$~вероятностная модель шифра.

В большинстве случаев множества $M$ и $C$ представляют собой объединение декартовых степеней некоторых алфавитов $A$ и $B$ (например, двоичный алфавит), так что для некоторых натуральных  $N_1$ и  $N_2$:

\begin{equation*}
М=\bigcup_{i=1}^{N_1}{A^i}, C=\bigcup_{i=1}^{N_2}{B^i}
\end{equation*}

\section{Информационная энтропия}
Американский математик Клод Шеннон в середине  двадцатого века ввел в теорию информации понятие информационной энтропии. Если читающему эти строки сказать его имя или сказать, что в неделе семь дней, то, очевидно, он не получит никакой информации. Если же он, например, размышляет над неопределенностью: брать с собой утром на работу зонт от дождя или нет, -- то он смотрит прогноз погоды и получает информацию, на основании которой делает решение. Полученная информации устраняет неопределенность. Введенное Шенноном понятие энтропии основывается на идее равенства количества полученной информации и количества устраненной неопределенности. Если под неопределенностью понимать случайную величину, значение которой есть результат опыта, то энтропия служит для измерения количества информации, содержащейся в случайной величине. Пусть случайная величина $\xi$ задана распределением вероятностей
$$\begin{tabular}[t]{|c|c|c|c|c|}
	\hline
	$\xi=$ & $x_1$ & $x_2$ & $\ldots$ & $x_n$  \\ 
	\hline
	$p=$ & $p_1$ & $p_2$ &$\ldots$ & $p_n$  \\ 
	\hline
\end{tabular}
$$
Тогда её энтропия (неопределенность, количество информации в ней) вычисляется по формуле  
$$H(\xi)=-\sum_{i=1}^{n}{p_i\log_2(p_i)}.$$
Единицей количества информации считают 1 бит. Например, количество информации, получаемое при бросании монеты, равно 1 бит. 
Приведём несколько свойств энтропии:
\begin{enumerate}
\item  $0\leq H(\xi)\leq\log_2(n)$,
\item  $H(\xi)=0$ тогда и только тогда, когда случайная величина может принимать одно единственное значение,
\item  $H(\xi)=\log_2(n)$ тогда и только тогда, когда  $p_i=\frac{1}{n}$ при всех $i\in\{1,2,\dots,n\}$.
\end{enumerate}

Пусть случайная величина $\zeta$ имеет распределение вероятностей:
$$\begin{tabular}[t]{|c|c|c|c|c|}
	\hline
	$\zeta=$ & $y_1$ & $y_2$ & $\ldots$ & $y_m$  \\ 
	\hline
	$p=$ & $q_1$ & $q_2$ &$\ldots$ & $q_m$  \\ 
	\hline
\end{tabular}
$$

Тогда неопределенность случайной величины $\xi$ при условии, что случайная величина $\zeta$ равна $y_j$ вычисляется по формуле
$$H(\xi|\zeta=y_j)=-\sum_{i=1}^{n}{p(\xi=x_i|\zeta=y_j)\log_2(p(\xi=x_i|\zeta=y_j))}.$$
Математическое ожидание величины
$$\begin{tabular}[t]{|c|c|c|c|c|}
	\hline
	$H=$ & $H(\xi|\zeta=y_1)$ & $H(\xi|\zeta=y_2)$ & $\ldots$ & $H(\xi|\zeta=y_m)$  \\ 
	\hline
	$p=$ & $q_1$ & $q_2$ &$\ldots$ & $q_m$  \\ 
	\hline
\end{tabular}
$$
называется условной энтропией опыта $\xi$ при условии выполнения опыта $\zeta$. Она обозначается через $H(\xi|\zeta)$ и вычисляется по формуле
$$H(\xi|\zeta)=\sum_{j=1}^{m}q_jH(\xi|\zeta=y_j).$$
Величина $I(\zeta,\xi)=H(\xi)-H(\xi|\zeta)$ показывает, сколько содержится информации в опыте $\zeta$ об опыте или относительно опыта $\xi$

Если величины $\xi$ и $\zeta$ независимы, то можно доказать, что $H(\xi)=H(\xi|\zeta)$ и, следовательно, $I(\zeta,\xi)=0$. 

\section{Энтропия и избыточность языка}

Мерой среднего количества информации, приходящейся на одну букву отрытого теста языка  $L$ (например, русского), служит величина $H_L$, называемая энтропией языка  $L$. Пусть в алфавите языка  $L$  $n$ букв. Пусть  $H_0=\log_2(n)$, а  $H_r$ энтропия вероятностного распределения  $r$-грамм в языке. Например, для русского языка $H_1=H(\xi)$ вычисляется на основе следующей таблицы относительных частот русских букв:  
$$
\begin{tabular}[t]{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$\xi$ & $p$ & $\xi$ & $p$ &$\xi$ & $p$ &$\xi$ & $p$ &$\xi$ & $p$ &$\xi$ & $p$ \\
\hline
А&0,081&Ж&0,08&М&0,032&Т&0,063&Ш&0,007&Ю&0,006\\
Б&0,015&З&0,019&Н&0,067&У&0,027&Щ&0,005&Я&0,002\\
В&0,044&И&0,075&О&0,11&Ф&0,004&Ъ&0,001&&\\
Г&0,016&Й&0,012&П&0,03&Х&0,009&Ы&0,018&&\\
Д&0,028&К&0,034&Р&0,049&Ц&0,006&Ь&0,015&&\\
Е&0,083&Л&0,046&С&0,054&Ч&0,014&Э&0,002&&\\
\hline
\end{tabular}
$$
$H_2=H(\xi)$ вычисляется на основе аналогичной таблицы частот биграмм: 
$$\begin{matrix}\xi &{АА}&{АБ}&{\dots}&\mathit{ЯЯ}\\p&{0,0004}&{0,00024}&{\dots}&{0,0002}\end{matrix}.$$ Тогда $H_L$ естественно вычислять ее последовательными приближениями:  $H_0$,  $H_1$,  $\frac{H_2}{2}$,  $\frac{H_3}{3}$, $\ldots$. Соответствующие вычисления для русского языка дают значения $H_0\approx 5$,  $H_1 \approx 4,35$,  $\frac{H_2}{2}\approx 3,52$,  $\frac{H_3}{3}\approx 3,01$, и так далее. Исследования показывают, что с ростом $r$ отношение $\frac{H_r}{r}$ стремится к некоторому пределу. Этот предел и принимается за определение энтропии $H_L$ языка $L$: 
$$H_L=\lim_{r\rightarrow\infty} H_r.$$
При этом формула
\begin{equation*}
R_L=1-\frac{H_L}{\log_2(n)}
\end{equation*}
определяет избыточность языка. Термин избыточность языка основывается на идее, что максимальная информация, которую в принципе могла бы нести каждая буква сообщения, равна  $H_0=\log_2(n)$. Но так было бы верно в случае, если буквы сообщения появлялись равномерно: случайно и равновероятно. На самом деле, буква несет меньше информации чем $H_0=\log_2(n)$. Например, в частичном слове ``мат-матика'', в котором потерялась четвертая буква, эта отсутствующая буква не несёт никакой информации. Величина $\log_2(n)-H_L$ характеризует, таким образом, неиспользованные возможности в передаче информации с помощью текста, а величина $R_L$ в некотором смысле показывает, какую часть букв открытого текста можно опустить без потери содержания (то есть с возможностью её восстановления).

В справочниках можно встретить следующие значения для русского языка:
$$
\begin{tabular}{|c|c|c|c|c|}
\hline
~ & {Энтропия языка} & {Избыточность}\\
~ & (бит/букву) & (в процентах) \\
\hline
{Язык в целом} & 1,37 & 72,6 \\
\hline
{Разговорная речь} & 1,4 & 72\\
\hline
{Литературный текст} & 1,19 &  76,2 \\
	\hline
{Деловой текст} &  0,83 & 83,4 \\
\hline
\end{tabular}
$$
Из приведенной таблицы видно, что русский язык имеет большую избыточность. Понятно, что избыточностью обладают и другие естественные языки. Избыточность 75\% не означает буквально то, что любые $3$ из $4$ букв текста можно вычеркнуть без потери информации. Это скорее означает, что при оптимальном кодировании текста его можно сжать до четверти длины без потери информации. К примеру, этим пользуются для сжатия информации архиваторы.


\section{Совершенный или абсолютно стойкий шифр}

\begin{definition}
Шифр $S=(M,C,K,e,d,F_M,F_K)$ называется совершенным, если для любого сообщения $m\in M$ и любой криптограммы $c\in C$ выполняется равенство $p(\xi_M=m|\xi_C=c)=p(\xi_M=m)$.
\end{definition}
Если шифр совершенный, то это значит, что зашифрованные сообщения не меняют распределение вероятностей открытых текстов. Другими словами, криптограммы не дают никакой информации о зашифрованном сообщении.

\begin{theorem} (К.Шеннон)
Если $|M|=|C|=|K|$, то шифр $S=(M,C,K,e,d,F_M,F_K)$ абсолютно стойкий тогда и только тогда, когда одновременно выполняются следующие свойства:
\begin{enumerate}
	\item для любых $k\in K$ $F_K(k)=p(\xi_K=k)=1/|K|$,
	\item для любой пары $m\in M$ и $c\in C$ существует единственный ключ $k\in K$ такой, что $e(m,k)=c$.
\end{enumerate}
\end{theorem}
  
